<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="EAM: Embodiment Agnostic Long-Horizon Manipulation using Human-Play
Data">
  <meta name="keywords" content="VLMs, Natural Language Feedback, Dense Reward, TAMER, RoboCLIP">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>EAM: Embodiment Agnostic Long-Horizon Manipulation using Human-Play
Data</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <!-- <link rel="icon" href="./static/images/favicon.svg"> -->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <!-- <a class="navbar-item" href="https://keunhong.com">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a> -->


    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">EAM: Embodiment Agnostic Long-Horizon Manipulation using Human-Play
Data</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://dhruv2012.github.io/">Dhruv Patel</a><sup>*</sup>,</span>
            <span class="author-block">
              <a href="">Pranay Mathur</a><sup>*</sup></span>
            <!-- <span class="author-block">
              <a href="">Kurt Contreras Diaz</a><sup>*</sup>,</span>
            </span>
            <span class="author-block">
              <a href="">Matthew Gombolay</a>
            </span> -->
          </div>

          <div class="is-size-5 publication-authors">
            <!-- <span class="author-block"><sup>1</sup>University of Washington,</span> -->
            <span class="author-block">Georgia Institute of Technology<span>
              <br>
            <span class="author-block">*Equal Contribution</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href="https://drive.google.com/file/d/1ViIyGmH46d83LfSeX_WottIvGRy_7NO-/view?usp=sharing"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Report</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href="https://arxiv.org/abs/2011.12948"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link. -->
              <!-- <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span> -->
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>



<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          Can we learn a hierarchical visuomotor control policy
          that can generalize to novel scenes, objects and geometries
          without scaling teleoperated robot demonstrations ? Recent
          works have shown impressive performance on manipulation
          tasks through learning policies leveraging robot teleopera-
          tion data collected at scale. To ensure true autonomy in
          the real-world these policies should be able to generalize to
          multiple tasks, visual domains, and diverse object geome-
          tries in unstructured environments. A scalable solution must
          reduce the dependence on collecting a large number of tele-
          operated demonstrations while simultaneously ensuring the
          alternative can be used to learn a representation that guides
          low-level control effectively. We propose learning a policy
          using human-play data - trajectories of humans freely inter-
          acting with their environment. Human-play data provides
          rich guidance about high-level actions to the low-level con-
          troller. We demonstrate the effectiveness of our high-level
          policy by testing with low-level control methods that use few
          teleoperation demos. Further, we examine the feasibility
          of a hierarchical policy that requires no teleoperation data
          and can generalize to any robot embodiment while obeying
          the kinematic constraints of the embodiment. We present
          our results and ablation studies on tasks evaluated in the
          real-world.
        </div>
      </div>
    </div>
    <!--/ Abstract. -->

    <!-- Paper video. -->
    <!-- <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Video</h2>
        <div class="publication-video">
          <iframe src="https://www.youtube.com/embed/MrKrnHhk8IA?rel=0&amp;showinfo=0"
                  frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>
        </div>
      </div>
    </div> -->
    <!--/ Paper video. -->
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Motivation</h2>
        <div class="content has-text-justified" style="display: flex; align-items: flex-start;">
          Can we generalize robot learning policies to unstructured scenes, novel objects and diverse motions without collecting millions of teleoperated trajectories ?
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Learning from Visual Demonstrations</h2>
        <div class="content has-text-justified">
          <h3 class="title is-4">Imitation Learning</h3>
          <ul>
            <li>Uses expert demonstrations to train robots to perform tasks under conditions of minor variations</li>
          </ul>
          <h3 class="title is-4">Challenges to LfD</h3>
          <ul>
            <li>Abundant teleoperation data requirement - <span style="color: red;">Tedious, Time-consuming</span></li>
            <li><span style="color: red;">Poor generalization</span> to novel visual domains</li>
            <li><span style="color: red;">Embodiment gap</span> with different manipulators</li>
          </ul>
        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-centered">Our Approach</h2>
        <figure class="image has-text-centered">
          <img src="./static/images/approach.png" alt="Our Approach Image" style="height: 120%">
        </figure>

        <br>
        <h3 class="title is-4 has-text-centered">Bridging the embodiment gap</h3>
        <p>We experiment with multiple techniques for bridging the human-robot embodiment gap as below:</p>
        <ul style="list-style-type: disc; padding-left: 20px; text-align: left;">
          <li>DINOV2 + LoRA (Rein)</li>
          <li>KL Divergence</li>
          <li>Masking Manipulator</li>
          <li>Co-training with robot play data + Depth data</li> 
          <li>Overlaying corresponding joints on hand and robot</li>
        </ul>
        <br>
        <div style="display: flex; justify-content: space-around;">
          <img src="./static/images/embodiment-1.png" alt="Embodiment 1" style="max-width: 45%; height: auto;">
          <img src="./static/images/embodiment-2.png" alt="Embodiment 2" style="width: 55%; height: 35%; margin-top: 120px;">
        </div>

        <br><br>
        <h3 class="title is-4 has-text-centered">Low-level Policy: Action Chunking with Transformers (ACT) </h3>
        <figure style="max-width: 100%; height: auto; justify-content: space-around;">
          <img src="./static/images/act.png" alt="Embodiment 1" style="width: 100%; height: auto;">
          <figcaption>Learning Fine-Grained Bimanual Manipulation with Low-Cost Hardware, Zhao et al. 2023</figcaption>
        </figure>
        <p></p>
        

      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Results</h2>
        <div class="content has-text-justified">

          <h3>Human Play Data</h3>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-videos" style="display: flex; justify-content: space-between; gap: 10px;">
                <video id="dollyzoom" autoplay controls muted loop playsinline style="flex: 1; height: 200px;">
                  <source src="./static/videos/cotrained_hand.mp4"
                          type="video/mp4">
                </video>
                <video id="dollyzoom" autoplay controls muted loop playsinline style="flex: 1; height: 200px;">
                  <source src="./static/videos/gray_trained_checkerboard_eval.mp4"
                          type="video/mp4">
                </video>
                <video id="dollyzoom" autoplay controls muted loop playsinline style="flex: 1; height: 200px;">
                  <source src="./static/videos/gray_trained_gray_eval.mp4"
                          type="video/mp4">
                </video>
                
                <!-- <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="./static/videos/cotrained_hand.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 200px;"></iframe>
                </div>
                <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="./static/videos/gray_trained_checkerboard_eval.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 200px;"></iframe>
                </div>
                <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="./static/videos/gray_trained_gray_eval.mp4" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 200px;"></iframe>
                </div> -->
              </div>
              High-level policy evaluation on human-play data. Ground truth (Green) and Prediction (Purple)
            </div>
          </div>

          <h3>Qualitative</h3>
          <div class="columns is-centered has-text-centered">
            <div class="column is-four-fifths">
              <div class="publication-videos" style="display: flex; justify-content: space-between; gap: 10px;">
                <iframe src="https://drive.google.com/file/d/1CW4R4301MbhF1f2GNg6Kg8pZHYCKXS3o/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="flex: 1; height: 300px;"></iframe>
                <iframe src="https://drive.google.com/file/d/1Cd7ZKqY6Qbch-xLitkvMXM8xjo11coGd/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="flex: 1; height: 300px;"></iframe>
                <iframe src="https://drive.google.com/file/d/1Cige_7WQQ5Snp8Yk4fGw751pvm5BgEol/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="flex: 1; height: 300px;"></iframe>

                <!-- <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="https://drive.google.com/file/d/1CW4R4301MbhF1f2GNg6Kg8pZHYCKXS3o/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 150px;"></iframe>
                </div>
                <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="https://drive.google.com/file/d/1Cd7ZKqY6Qbch-xLitkvMXM8xjo11coGd/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 150px;"></iframe>
                </div>
                <div class="publication-video" style="flex: 1; margin: 0 10px;">
                  <iframe src="https://drive.google.com/file/d/1Cige_7WQQ5Snp8Yk4fGw751pvm5BgEol/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="width: 100%; height: 150px;"></iframe>
                </div> -->
              </div>
              <p>Our high-level policy combined with ACT was able to demonstrate object and scene level generalization</p>

              <br>
              <h4> Robustness and real-time replanning</h4>
              <div class="publication-videos" style="display: flex; justify-content: space-between; gap: 10px;">
                <iframe src="https://drive.google.com/file/d/1Ib6LaCEhlnHlbalI0neOwLwYm2c5J9Hn/preview" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen style="flex: 1; height: 300px;"></iframe>
              </div>
              <figure style="max-width: 100%; height: auto; justify-content: space-around;">
                <img src="./static/images/realtimereplanning.png" alt="Embodiment 1" style="width: 100%; height: auto;">
                <figcaption>Real-time replanning</figcaption>
              </figure>
              <br>
            </div>
          </div>

          <h3>Quantitative</h3>

          <figure style="max-width: 100%; height: auto; justify-content: space-around;">
            <img src="./static/images/highlevelablations.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <figcaption>DINOV2 with Low-rank Adaptation-based vision encoder achieves the best performance in terms of distance-metric</figcaption>
          </figure>
          <figure style="max-width: 100%; height: auto; justify-content: space-around;">
            <img src="./static/images/lowlevelablations.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <figcaption>Top-view along with wrist cameras achieve best success rates on unseen objects by leveraging style and latent information from ACT encoder</figcaption>
          </figure>
          <figure style="max-width: 100%; height: auto; justify-content: space-around;">
            <img src="./static/images/scenelevel.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <figcaption>High-level policy enables scene-level generalization</figcaption>
          </figure>
          <figure style="max-width: 100%; height: auto; justify-content: space-around;">
            <img src="./static/images/ablations.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <!-- <figcaption>EAM high-level with ACT low-level achieve best success rate by learning embodiment-agnostic representations using KL loss-based optimization. 
              This also enables comparable success rates to other approaches by integrating high-level policy with Gradient-based IK solver for joint-space prediction
            </figcaption> -->
            <ul style="text-align: left;">
              <li>EAM high-level with ACT low-level achieve best success rate by learning embodiment-agnostic representations using KL loss-based optimization.</li>
              <li>This also enables comparable success rates to other approaches by integrating high-level policy with Gradient-based IK solver for joint-space prediction</li>
            </ul>
          </figure>
          <figure style="max-width: 100%; height: auto; justify-content: space-around;">
            <img src="./static/images/evaluations.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <figcaption>EAM+ACT using data from both human and robot embodiment achieve best success rate.</figcaption>
          </figure>
          <!-- Add content here -->
          <!-- <table>
            <tr>
              <th>Model</th>
              <th>Vision Encoder</th>
              <th>Training</th>
              <th>Final MSE (cm)</th>
              <th>Paired MSE (cm)</th>
            </tr>
            <tr>
              <td>EAM</td>
              <td>ResNet</td>
              <td>Robot</td>
              <td>44.901</td>
              <td>28.774</td>
            </tr>
            <tr>
              <td>EAM</td>
              <td>DinoV2</td>
              <td>Robot</td>
              <td>19.091</td>
              <td>19.281</td>
            </tr>
            <tr>
              <td>EAM</td>
              <td>DinoV2+LoRA</td>
              <td>Robot</td>
              <td>8.459</td>
              <td>6.169</td>
            </tr>
            <tr>
              <td>EAM</td>
              <td>DinoV2+LoRA</td>
              <td>Robot+Human (Co-training)</td>
              <td>30.418</td>
              <td>20.981</td>
            </tr>
            <tr>
              <td>EAM+KL</td>
              <td>DinoV2+LoRA</td>
              <td>Robot+Human (Co-training)</td>
              <td>19.644</td>
              <td>14.855</td>
            </tr>
            <tr>
              <td>EAM+Segmentation</td>
              <td>DinoV2+LoRA</td>
              <td>Robot+Human (Co-training)</td>
              <td>47.319</td>
              <td>50.229</td>
            </tr>
            <tr>
              <td>EAM+DomainDiscriminator</td>
              <td>DinoV2+LoRA</td>
              <td>Robot+Human (Co-training)</td>
              <td>30.452</td>
              <td>34.671</td>
            </tr>
          </table> -->

          <br>
          <br>
        </div>
      </div>
    </div>
  </div>
</section>



<style>
  .image-row {
    display: flex;
    justify-content: center;
    margin-bottom: 10px;
  }
  .image-row img {
    margin: 0 10px;
    width: 500px; /* Adjust the width as needed */
    height: auto; /* Maintain aspect ratio */
  }
  .caption {
    text-align: center;
    margin-top: 20px;
    font-style: italic;
  }
</style>







<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Future Work</h2>
        <div class="columns is-vcentered" style="display: flex; justify-content: space-between; align-items: flex-start;">
          <figure style="max-width: 48%; height: auto; margin: 0;">
            <img src="./static/images/clip.png" alt="Embodiment 1" style="width: 100%; height: auto;">
            <figcaption>Overlaying CLIP attention on input image. Prompt: "Toy, Bowl and Hand".</figcaption>
          </figure>
          <div style="max-width: 48%; margin: 0; text-align: left;">
            <ul style="list-style-position: inside; padding-left: 0;">
              <li>- Non-Hierarchical single stage policy to remove need for low-level teleoperation data</li>
              <li>- Extending to cluttered environments and more complex motion trajectories</li>
              <li>- Using CLIP to allow goal specification through language and provide better task specific scene abstraction</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>




<!-- <section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section> -->


<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This website borrows the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
